{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion of Lausanne grid data from Lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T08:10:29.120977Z",
     "start_time": "2025-03-13T08:10:27.865119Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='[%(levelname)s] %(funcName)s: %(message)s',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T16:14:52.229887Z",
     "start_time": "2025-05-01T16:14:52.126514Z"
    }
   },
   "outputs": [],
   "source": [
    "shared_folder = '/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025'\n",
    "grid_path = os.path.join(shared_folder, 'data/sebeillon-node-loc.csv')\n",
    "buildings_path = '/Users/catarina/switchdrive/IPESE-DESL_data_exchange/Weather/CRYOS_data/buildings_cryos.gpkg'\n",
    "\n",
    "def generate_gdf(point_path, long_col, lat_col, input_crs):\n",
    "    \"\"\"\n",
    "    Function to generate a geodataframe from a csv file with point data.\n",
    "    Parameters:\n",
    "    - point_path: str, path to the csv file\n",
    "    - long_col: str, name of the column with the longitude data\n",
    "    - lat_col: str, name of the column with the latitude data\n",
    "    - crs: int, EPSG code of the coordinate reference system\n",
    "    Returns:\n",
    "    - gdf: geopandas geodataframe\n",
    "    \"\"\"\n",
    "    # check if extension is csv\n",
    "    if Path(point_path).suffix == '.csv':\n",
    "        # read csv\n",
    "        df = pd.read_csv(point_path)\n",
    "        # create geometry column\n",
    "        df['geometry'] = gpd.points_from_xy(df[long_col], df[lat_col])\n",
    "        # create geodataframe\n",
    "        gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=input_crs)\n",
    "        return gdf\n",
    "    else:\n",
    "        print('File extension not supported. Please provide a csv file.')\n",
    "\n",
    "def generate_grid_mask(grid_gdf, offset, crs_output):\n",
    "    \"\"\"\n",
    "    Function to generate a polygon mask from a line that encircles the points from a GeoDataFrame plus an offset.\n",
    "    \n",
    "    Parameters:\n",
    "    - grid_gdf: geopandas GeoDataFrame, grid points\n",
    "    - offset: int, buffer offset in meters\n",
    "    - crs_output: int, EPSG code of the output coordinate reference system\n",
    "    \n",
    "    Returns:\n",
    "    - mask: geopandas GeoDataFrame, polygon mask of the grid\n",
    "    \"\"\"\n",
    "    # Create a convex hull around the points\n",
    "    hull = unary_union(grid_gdf.geometry).convex_hull\n",
    "\n",
    "    # Apply buffer (offset in meters)\n",
    "    mask_polygon = hull.buffer(offset)\n",
    "\n",
    "    # Convert back to the original CRS if needed\n",
    "    mask_gdf = gpd.GeoDataFrame(geometry=[mask_polygon], crs=grid_gdf.crs)\n",
    "    \n",
    "    # Reproject to the specified output CRS if necessary\n",
    "    if crs_output is not None and mask_gdf.crs.to_epsg() != crs_output:\n",
    "        mask_gdf = mask_gdf.to_crs(epsg=crs_output)\n",
    "\n",
    "    return mask_gdf\n",
    "\n",
    "def clean_qbuildings(buildings_path,crs):\n",
    "    \"\"\"\n",
    "    Checks if there are empty columns and nan values that might give errors when running reho, and removes them.\n",
    "    If necessary, converts the coordinate's reference system to the one from the temperature file\n",
    "    Returns:\n",
    "        buildings_gdf: Cleaned buildings GeoDataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # read buildings\n",
    "    buildings_gdf = gpd.read_file(buildings_path)\n",
    "\n",
    "    # Initial row count\n",
    "    build_ini = buildings_gdf.shape[0]\n",
    "    logging.info(f'reading file with {build_ini} buildings')\n",
    "    #buildings_gdf.to_csv('buildings_ini.csv')\n",
    "\n",
    "    # Check for completely empty columns\n",
    "    empty_columns = buildings_gdf.columns[buildings_gdf.isna().all()].tolist()\n",
    "    if empty_columns:\n",
    "        buildings_gdf = buildings_gdf.drop(columns=empty_columns)\n",
    "        logging.warning(f'Dropped empty columns: {empty_columns}')\n",
    "\n",
    "    # Check for NaN values\n",
    "    nan_dict = {}\n",
    "    for row_index in range(len(buildings_gdf)):\n",
    "        row = buildings_gdf.loc[row_index].isna() # checks each column in the specified row for NaN values\n",
    "        nan_col = row[row].index.tolist()  # Get columns with NaN\n",
    "        if nan_col:\n",
    "            id_building = buildings_gdf['id_building'].loc[row_index]\n",
    "            nan_dict[id_building] = nan_col  # Add id_building and NaN columns to dictionary\n",
    "\n",
    "    # Drop buildings with NaN values\n",
    "    # Get the list of id_building values to drop from nan_dict\n",
    "    ids_to_drop = list(nan_dict.keys())\n",
    "    # Drop rows where id_building is in ids_to_drop\n",
    "    buildings_gdf = buildings_gdf[~buildings_gdf['id_building'].isin(ids_to_drop)]\n",
    "    build_drop = build_ini - buildings_gdf.shape[0]\n",
    "    logging.warning(f'{build_drop} buildings with NaN values dropped')\n",
    "\n",
    "    # Extract the CRS\n",
    "    crs_ini = buildings_gdf.crs\n",
    "\n",
    "    if crs_ini != crs:\n",
    "        buildings_gdf = buildings_gdf.to_crs(crs)\n",
    "        logging.warning(f'Converted CRS from {crs_ini} to {crs}')\n",
    "    else:\n",
    "        logging.info(f'CRS: {crs_ini}')\n",
    "\n",
    "    return buildings_gdf\n",
    "\n",
    "def apply_grid_mask(buildings_gdf,grid_mask):\n",
    "        \"\"\"\n",
    "        Function to apply a grid mask to a GeoDataFrame of buildings.\n",
    "        Parameters:\n",
    "            buildings_gdf: GeoDataFrame with buildings\n",
    "            grid_mask: GeoDataFrame with the grid\n",
    "        Returns:\n",
    "            buildings_gdf: GeoDataFrame with buildings inside the grid mask\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure both GeoDataFrames use the same CRS\n",
    "        if buildings_gdf.crs != grid_mask.crs:\n",
    "            crs_ini = buildings_gdf.crs\n",
    "            buildings_gdf = buildings_gdf.to_crs(grid_mask.crs)\n",
    "            logging.warning(f'Converted buildings CRS from {crs_ini} to {buildings_gdf.crs}')\n",
    "\n",
    "        # Compute centroids of the buildings\n",
    "        centroids = buildings_gdf.copy()\n",
    "        centroids[\"geometry\"] = buildings_gdf.geometry.centroid\n",
    "\n",
    "        # Perform spatial join using \"within\" to keep buildings whose centroids are inside the grid mask\n",
    "        filtered_centroids = gpd.sjoin(centroids, grid_mask, predicate=\"within\", how=\"inner\")\n",
    "\n",
    "        # Select original building geometries that correspond to the filtered centroids\n",
    "        filtered_gdf = buildings_gdf.loc[filtered_centroids.index]\n",
    "\n",
    "        # Ensure the result is a GeoDataFrame\n",
    "        filtered_buildings_gdf = gpd.GeoDataFrame(filtered_gdf, geometry='geometry', crs=buildings_gdf.crs)\n",
    "\n",
    "        return filtered_buildings_gdf\n",
    "\n",
    "grid = generate_gdf(grid_path,'x','y',2056)\n",
    "#grid.to_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/grid.gpkg', driver='GPKG')\n",
    "#grid_mask = generate_grid_mask(grid, 100,2056)\n",
    "#buidings_cleaned = clean_qbuildings(buildings_path,2056)\n",
    "#filtered_buildings = apply_grid_mask(buidings_cleaned,grid_mask)\n",
    "#grid_mask.to_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/grid_mask.gpkg', driver='GPKG')\n",
    "#filtered_buildings.to_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/buildings_SDEWES.gpkg', driver='GPKG')\n",
    "#filtered_buildings.to_csv('/Users/catarina/Documents/REHO/scripts/Pathways/QBuildings/buildings_SDEWES.csv')\n",
    "\n",
    "lausanne_mask = gpd.read_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/Weather/CRYOS_data/Lausanne mask.gpkg')\n",
    "#buildings_lausanne = apply_grid_mask(buidings_cleaned,lausanne_mask)\n",
    "#buildings_lausanne.to_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/data/buildings_lausanne.gpkg', driver='GPKG')\n",
    "#buildings_lausanne.to_csv('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/data/buildings_lausanne.csv')\n",
    "filtered_buildings = gpd.read_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/data/buildings_SDEWES.gpkg')\n",
    "\n",
    "grid_mask = gpd.read_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/data/grid_mask.gpkg')\n",
    "fig_grid, ax = plt.subplots()\n",
    "#filtered_buildings.plot(ax=ax, color='blue', markersize=1)\n",
    "filtered_buildings.plot(ax=ax, color='blue', markersize=1)\n",
    "lausanne_mask.plot(ax=ax, edgecolor='grey', facecolor='none')\n",
    "grid.plot(ax=ax, color='red', markersize=1)\n",
    "grid_mask.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "\n",
    "ax.set_xlabel('Longitude [째]')\n",
    "ax.set_ylabel('Latitude [째]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add installed PV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='[%(levelname)s] %(funcName)s: %(message)s',\n",
    "                    )\n",
    "\n",
    "electricity_plants_path = '/Users/catarina/Library/CloudStorage/OneDrive-epfl.ch/1.Projects & Colaborations/UrbanTwin/18.Data/Electricity generation plants/052025_ch.bfe.elektrizitaetsproduktionsanlagen.gpkg'\n",
    "electricity_csv_path = '/Users/catarina/Library/CloudStorage/OneDrive-epfl.ch/1.Projects & Colaborations/UrbanTwin/18.Data/Electricity generation plants/electricity_production_plants.csv'\n",
    "\n",
    "plant_df = pd.read_csv(electricity_csv_path)\n",
    "\n",
    "# list all layers in the file\n",
    "layers = fiona.listlayers(electricity_plants_path)\n",
    "\n",
    "# Read each layer separately into a GeoDataFrame\n",
    "gdfs = {layer: gpd.read_file(electricity_plants_path, layer=layer) for layer in layers}\n",
    "\n",
    "lausanne_mask = gpd.read_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/Weather/CRYOS_data/Lausanne mask.gpkg')\n",
    "\n",
    "# ensure both GeoDataFrames use the same CRS\n",
    "if gdfs['ElectricityProductionPlant'].crs != lausanne_mask.crs:\n",
    "    crs_ini = gdfs['ElectricityProductionPlant'].crs\n",
    "    gdfs['ElectricityProductionPlant'] = gdfs['ElectricityProductionPlant'].to_crs(lausanne_mask.crs)\n",
    "    logging.warning(f'Converted plant CRS from {crs_ini} to {gdfs[\"ElectricityProductionPlant\"].crs}')\n",
    "\n",
    "# Add a 2-meter buffer to the lausanne_mask (expanding it by 100 meters), to include the plants that are very close to the border\n",
    "# Create a buffered version of the lausanne_mask\n",
    "lausanne_mask_buffered = lausanne_mask.buffer(100)\n",
    "# Combine buffered geometries into a single shape (if needed)\n",
    "lausanne_mask_buffered = gpd.GeoDataFrame(geometry=[lausanne_mask_buffered.unary_union], crs=lausanne_mask.crs)\n",
    "\n",
    "# Perform spatial join using \"within\" to keep plants inside the grid mask. Ignore index of lausanne_mask_buffered\n",
    "filtered_plants = gpd.sjoin(gdfs['ElectricityProductionPlant'], lausanne_mask_buffered, predicate=\"within\", how=\"inner\")\n",
    "\n",
    "# Drop the 'index_right' column added by the join\n",
    "filtered_plants = filtered_plants.drop(columns='index_right')\n",
    "\n",
    "fig_grid, ax = plt.subplots()\n",
    "#filtered_buildings.plot(ax=ax, color='blue', markersize=1)\n",
    "lausanne_mask.plot(ax=ax, edgecolor='grey', facecolor='none')\n",
    "lausanne_mask_buffered.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "#grid.plot(ax=ax, color='red', markersize=1)\n",
    "filtered_plants.plot(ax=ax, color='green', markersize=1)\n",
    "\n",
    "ax.set_xlabel('Longitude [째]')\n",
    "ax.set_ylabel('Latitude [째]')\n",
    "plt.show()\n",
    "\n",
    "# get dictionaries from the other layers\n",
    "# convert dataframe to dictionary\n",
    "MainCategory_df = gdfs['MainCategoryCatalogue']\n",
    "# keep columns 'Catalogue_id and 'en'\n",
    "MainCategory_df = MainCategory_df[['Catalogue_id', 'en']]\n",
    "\n",
    "MainCategory_dict = {}\n",
    "for index, row in MainCategory_df.iterrows():\n",
    "    # Create a dictionary entry with 'Catalogue_id' as key and 'en' as value\n",
    "    MainCategory_dict[row['Catalogue_id']] = row['en']\n",
    "\n",
    "SubCategory_df = gdfs['SubCategoryCatalogue']\n",
    "# keep columns 'Catalogue_id and 'en'\n",
    "SubCategory_df = SubCategory_df[['Catalogue_id', 'en']]\n",
    "SubCategory_dict = {}\n",
    "for index, row in SubCategory_df.iterrows():\n",
    "    # Create a dictionary entry with 'Catalogue_id' as key and 'en' as value\n",
    "    SubCategory_dict[row['Catalogue_id']] = row['en']\n",
    "\n",
    "PlantCategory_df = gdfs['PlantCategoryCatalogue']\n",
    "# keep columns 'Catalogue_id and 'en'\n",
    "PlantCategory_df = PlantCategory_df[['Catalogue_id', 'en']]\n",
    "# Create a dictionary entry with 'Catalogue_id' as key and 'en' as value\n",
    "PlantCategory_dict = {}\n",
    "for index, row in PlantCategory_df.iterrows():\n",
    "    # Create a dictionary entry with 'Catalogue_id' as key and 'en' as value\n",
    "    PlantCategory_dict[row['Catalogue_id']] = row['en']\n",
    "\n",
    "filtered_plants['MainCategory'] = filtered_plants['MainCategory'].map(MainCategory_dict)\n",
    "filtered_plants['SubCategory'] = filtered_plants['SubCategory'].map(SubCategory_dict)\n",
    "filtered_plants['PlantCategory'] = filtered_plants['PlantCategory'].map(PlantCategory_dict)\n",
    "\n",
    "# Get the InitialPower and TotalPower from the plant_df\n",
    "filtered_plants['InitialPower'] = filtered_plants['xtf_id'].map(plant_df.set_index('xtf_id')['InitialPower'])\n",
    "filtered_plants['TotalPower'] = filtered_plants['xtf_id'].map(plant_df.set_index('xtf_id')['TotalPower'])\n",
    "\n",
    "# filter the rows to keep only the ones with the SubCategory 'Photovoltaic'\n",
    "pv_plants = filtered_plants[filtered_plants['SubCategory'] == 'Photovoltaic']\n",
    "\n",
    "# Save the filtered plants to a geopackage file\n",
    "filtered_plants.to_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/data/electricity_plants_Lausanne.gpkg', driver='GPKG')\n",
    "# Save the PV plants to a geopackage file\n",
    "pv_plants.to_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/data/Lausanne_PV.gpkg', driver='GPKG')\n",
    "pv_plants.to_csv('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/data/Lausanne_PV.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add installed PV to QBuildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# read data\n",
    "buildings_df = gpd.read_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/data/buildings_SDEWES.gpkg')\n",
    "pv_plants_df = gpd.read_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/data/Lausanne_PV.gpkg')\n",
    "\n",
    "# Ensure both GeoDataFrames use the same coordinate reference system\n",
    "pv_plants_df = pv_plants_df.to_crs(buildings_df.crs)\n",
    "\n",
    "# Spatial join: find which PV points fall within which buildings\n",
    "pv_with_buildings = gpd.sjoin(pv_plants_df, buildings_df, how='inner', predicate='within')\n",
    "\n",
    "# Sum PV power per building. Adjust 'pv_power_kW' to your actual power column name\n",
    "pv_power_by_building = pv_with_buildings.groupby('id_building')['TotalPower'].sum().reset_index()\n",
    "\n",
    "# Set up a dictionary for quick lookup of PV power per building\n",
    "pv_power_dict = pv_power_by_building.set_index('id_building')['TotalPower'].to_dict()\n",
    "\n",
    "# Set is_pv to True where the building has PV\n",
    "buildings_df['is_pv'] = buildings_df['id_building'].isin(pv_power_dict)\n",
    "\n",
    "# Set pv_power using the dictionary, use .get() to handle buildings with no PVs (default to 0)\n",
    "buildings_df['pv_power_kW'] = buildings_df['id_building'].map(pv_power_dict).fillna(0)\n",
    "\n",
    "# Save the updated buildings GeoDataFrame to a new file\n",
    "buildings_df.to_file('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/data/buildings_SDEWES_with_pv.gpkg', driver='GPKG')\n",
    "# Save the updated buildings GeoDataFrame to a CSV file\n",
    "buildings_df.to_csv('/Users/catarina/switchdrive/IPESE-DESL_data_exchange/SDEWES2025/data/buildings_SDEWES_with_pv.csv')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
